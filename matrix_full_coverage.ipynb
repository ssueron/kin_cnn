{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adc79d",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\nfrom rdkit.Chem import rdFingerprintGenerator\nfrom sklearn.utils.extmath import randomized_svd\nfrom sklearn.cluster import KMeans\nfrom collections import defaultdict\n\n# Load datasets and family mapping\ndf_pretrain_orig = pd.read_csv('datasets/chembl_pretraining.csv')\ndf_finetune_orig = pd.read_csv('datasets/pkis2_finetuning.csv')\ndf_families = pd.read_csv('uniprotkb_chembl_ID_kinomescan_cleaned.csv')\n\n# Find SMILES column in finetune dataset\nsmiles_col_finetune = None\nfor col in df_finetune_orig.columns:\n    if col.lower() == 'smiles':\n        smiles_col_finetune = col\n        break\nif smiles_col_finetune is None:\n    raise KeyError(\"No 'smiles' column found in pkis2_finetuning.csv\")\nif smiles_col_finetune != 'smiles':\n    df_finetune_orig['smiles'] = df_finetune_orig[smiles_col_finetune]\n    df_finetune_orig = df_finetune_orig.drop(columns=[smiles_col_finetune])\n\n# Create protein to family mapping\nprotein_to_family = dict(zip(df_families['KINOMEscan® Gene Symbol'], df_families['Family']))\n\n# Get protein columns\npretrain_proteins = [c for c in df_pretrain_orig.columns if c != 'smiles']\nfinetune_proteins = [c for c in df_finetune_orig.columns if c != 'smiles']\nall_proteins = list(set(pretrain_proteins + finetune_proteins))\n\n# Canonicalize SMILES\ndef canonicalize_smiles(s):\n    if pd.isna(s): return None\n    m = Chem.MolFromSmiles(s)\n    if m is None: return None\n    return Chem.MolToSmiles(m, isomericSmiles=False)\n\n# Compute scaffolds\ndef bm_scaf(s):\n    if pd.isna(s): return None\n    m = Chem.MolFromSmiles(s)\n    if m is None: return None\n    sc = MurckoScaffold.GetScaffoldForMol(m)\n    if sc is None or sc.GetNumAtoms()==0: return None\n    return Chem.MolToSmiles(sc, isomericSmiles=False)\n\n# Function to perform splitting with spectral clustering\ndef split_dataset(df, proteins, protein_to_family, forced_assignments=None):\n    scaf_id, scaf_vocab = pd.factorize(df['scaffold_smiles'], sort=True)\n    df['scaf_id'] = scaf_id\n    \n    grouped = df.groupby('scaf_id').indices\n    scaff_to_idx = {k: np.array(v, dtype=int) for k, v in grouped.items() if k >= 0}\n    \n    # Spectral clustering\n    mols = [Chem.MolFromSmiles(s) for s in df['canon_smiles']]\n    rep_idx = [v[0] for v in scaff_to_idx.values()]\n    gen = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=2048)\n    X = np.zeros((len(rep_idx), 2048), dtype=bool)\n    for r, i in enumerate(rep_idx):\n        if mols[i] is not None:\n            bv = gen.GetFingerprint(mols[i])\n            on = list(bv.GetOnBits())\n            if on: X[r, on] = True\n    \n    def jaccard_AB(A, B, chunk=2048):\n        B_u16 = B.astype(np.uint16); B_t = B_u16.T\n        bsum = B.sum(axis=1).astype(np.int32)\n        n = A.shape[0]\n        out = np.empty((n, B.shape[0]), dtype=np.float32)\n        for i in range(0, n, chunk):\n            Ab = A[i:i+chunk]\n            inter = Ab.astype(np.uint16) @ B_t\n            asum = Ab.sum(axis=1).astype(np.int32)[:, None]\n            union = asum + bsum[None, :] - inter\n            union = np.maximum(union, 1)\n            out[i:i+chunk] = inter / union\n        return out\n    \n    rng = np.random.default_rng(0)\n    n = X.shape[0]\n    m = min(1024, n)\n    L = rng.choice(n, size=m, replace=False)\n    B = X[L]\n    C = jaccard_AB(X, B, chunk=2048).astype(np.float32)\n    W = jaccard_AB(B, B, chunk=1024).astype(np.float32)\n    eps = 1e-6\n    evals, evecs = np.linalg.eigh(W + eps*np.eye(m, dtype=np.float32))\n    Winv = (evecs / evals) @ evecs.T\n    Winvsqrt = (evecs / np.sqrt(evals)) @ evecs.T\n    u = C.sum(axis=0)\n    v = Winv @ u\n    d = C @ v\n    s = (1.0 / np.sqrt(d + 1e-12)).astype(np.float32)\n    E = (C * s[:, None]) @ Winvsqrt\n    U, S, VT = randomized_svd(E, n_components=3, random_state=0)\n    Y = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n    cluster_labels = KMeans(n_clusters=3, n_init=10, random_state=0).fit_predict(Y)\n    \n    # Family coverage computation with protein tracking\n    all_families = set(protein_to_family.values())\n    family_counts = {}\n    scaffold_activities = {}\n    protein_to_scaffolds = defaultdict(set)\n    protein_measurements = defaultdict(int)\n    \n    for k, idx in scaff_to_idx.items():\n        sub = df.iloc[idx]\n        family_counts[k] = defaultdict(int)\n        scaffold_activities[k] = 0\n        \n        for _, row in sub.iterrows():\n            orig_idx = row['original_idx']\n            for prot in proteins:\n                if pd.notna(df.iloc[orig_idx][prot]):\n                    protein_to_scaffolds[prot].add(k)\n                    protein_measurements[prot] += 1\n                    if prot in protein_to_family:\n                        family = protein_to_family[prot]\n                        family_counts[k][family] += 1\n                        scaffold_activities[k] += 1\n    \n    total_per_family = defaultdict(int)\n    for k, families in family_counts.items():\n        for family, count in families.items():\n            total_per_family[family] += count\n    \n    rare_families = set([f for f in total_per_family if total_per_family[f] < 50])\n    critical_families = set([f for f in total_per_family if total_per_family[f] < 20])\n    \n    # Split assignment\n    n_scaff = len(scaff_to_idx)\n    ratios = np.array([0.8, 0.1, 0.1])\n    targets = (ratios * n_scaff).astype(int)\n    targets[0] = n_scaff - targets[1] - targets[2]\n    \n    assign = {k: None for k in scaff_to_idx.keys()}\n    split_sizes = np.zeros(3, dtype=int)\n    family_cov = {f: np.zeros(3, dtype=int) for f in all_families}\n    \n    # Apply forced assignments\n    if forced_assignments:\n        for k in scaff_to_idx.keys():\n            scaffold_smiles = scaf_vocab[k] if k >= 0 else None\n            if scaffold_smiles in forced_assignments:\n                assign[k] = 0\n                split_sizes[0] += 1\n                for f, c in family_counts[k].items():\n                    if c > 0:\n                        family_cov[f][0] += 1\n    \n    # Handle critical scaffolds\n    critical_scaffolds = []\n    for k in scaff_to_idx.keys():\n        if assign[k] is None:\n            has_critical = any(f in critical_families for f in family_counts[k])\n            if has_critical:\n                critical_scaffolds.append(k)\n    \n    for k in critical_scaffolds:\n        best_split = None\n        best_score = -1000\n        \n        for s in range(3):\n            if split_sizes[s] >= targets[s]:\n                continue\n            score = 0\n            for f in family_counts[k]:\n                if f in critical_families:\n                    if family_cov[f][s] == 0:\n                        score += 100\n                    elif family_cov[f][s] < 2:\n                        score += 50\n                elif f in rare_families:\n                    if family_cov[f][s] == 0:\n                        score += 10\n            \n            if score > best_score:\n                best_score = score\n                best_split = s\n        \n        if best_split is not None:\n            assign[k] = best_split\n            split_sizes[best_split] += 1\n            for f, c in family_counts[k].items():\n                if c > 0:\n                    family_cov[f][best_split] += 1\n    \n    # Order remaining scaffolds\n    remaining = [k for k in scaff_to_idx.keys() if assign[k] is None]\n    order = sorted(remaining, key=lambda k: (\n        min([total_per_family[f] for f, c in family_counts[k].items() if c > 0]) if family_counts[k] else 10**9,\n        -len(family_counts[k]),\n        cluster_labels[list(scaff_to_idx.keys()).index(k)]\n    ))\n    \n    # Assign remaining scaffolds\n    for k in order:\n        gains = []\n        fc = family_counts[k]\n        \n        for s in range(3):\n            g = 0\n            for f, c in fc.items():\n                if c > 0:\n                    if f in critical_families and family_cov[f][s] == 0:\n                        g += 1000\n                    elif f in rare_families and family_cov[f][s] == 0:\n                        g += 100\n                    elif family_cov[f][s] == 0:\n                        g += 10\n                    elif family_cov[f][s] < 3:\n                        g += 1\n            \n            over = 100 if split_sizes[s] >= targets[s] else 0\n            gains.append((g, -over, -split_sizes[s], s))\n        \n        s = max(gains)[-1]\n        assign[k] = s\n        split_sizes[s] += 1\n        for f, c in family_counts[k].items():\n            if c > 0:\n                family_cov[f][s] += 1\n    \n    # Calculate protein coverage\n    protein_coverage = {}\n    for prot in proteins:\n        if prot in protein_to_scaffolds:\n            scaffolds = protein_to_scaffolds[prot]\n            split_coverage = [False, False, False]\n            for scaf_id in scaffolds:\n                if scaf_id in assign and assign[scaf_id] is not None:\n                    split_coverage[assign[scaf_id]] = True\n            protein_coverage[prot] = split_coverage\n        else:\n            protein_coverage[prot] = [False, False, False]\n    \n    # Apply splits to molecules\n    mol_split = np.full(len(df), -1, dtype=int)\n    for k, s in assign.items():\n        idx = scaff_to_idx[k]\n        mol_split[idx] = s\n    \n    df['split'] = mol_split\n    \n    # Return split info\n    split_scaffolds = {\n        'train': set(scaf_vocab[k] for k, s in assign.items() if s == 0 and k >= 0),\n        'val': set(scaf_vocab[k] for k, s in assign.items() if s == 1 and k >= 0),\n        'test': set(scaf_vocab[k] for k, s in assign.items() if s == 2 and k >= 0)\n    }\n    \n    return df, split_scaffolds, protein_coverage, protein_measurements\n\n# Optimization loop to achieve 100% coverage\ndef optimize_protein_coverage(df_pretrain_orig, df_finetune_orig, pretrain_proteins, finetune_proteins, \n                            protein_to_family, target_coverage=1.0):\n    df_pretrain = df_pretrain_orig.copy()\n    df_finetune = df_finetune_orig.copy()\n    \n    df_pretrain['canon_smiles'] = df_pretrain['smiles'].apply(canonicalize_smiles)\n    df_finetune['canon_smiles'] = df_finetune['smiles'].apply(canonicalize_smiles)\n    \n    df_pretrain = df_pretrain[df_pretrain['canon_smiles'].notna()].reset_index(drop=True)\n    df_finetune = df_finetune[df_finetune['canon_smiles'].notna()].reset_index(drop=True)\n    \n    df_pretrain['original_idx'] = np.arange(len(df_pretrain))\n    df_finetune['original_idx'] = np.arange(len(df_finetune))\n    \n    df_pretrain['scaffold_smiles'] = df_pretrain['canon_smiles'].map(bm_scaf)\n    df_finetune['scaffold_smiles'] = df_finetune['canon_smiles'].map(bm_scaf)\n    \n    proteins_to_remove = set()\n    iteration = 0\n    \n    while iteration < 50:\n        iteration += 1\n        \n        current_pretrain_proteins = [p for p in pretrain_proteins if p not in proteins_to_remove]\n        current_finetune_proteins = [p for p in finetune_proteins if p not in proteins_to_remove]\n        \n        # Split pretraining\n        df_pretrain_split, pretrain_scaffolds, pretrain_coverage, pretrain_measurements = split_dataset(\n            df_pretrain.copy(), current_pretrain_proteins, protein_to_family\n        )\n        \n        # Identify critical scaffolds\n        finetune_scaffolds = set(df_finetune['scaffold_smiles'].dropna())\n        critical_scaffolds = pretrain_scaffolds['train'] & finetune_scaffolds\n        \n        # Split finetuning\n        df_finetune_split, finetune_scaffolds_split, finetune_coverage, finetune_measurements = split_dataset(\n            df_finetune.copy(), current_finetune_proteins, protein_to_family, \n            forced_assignments=critical_scaffolds\n        )\n        \n        # Calculate coverage metrics\n        pretrain_problematic = []\n        finetune_problematic = []\n        \n        for prot, cov in pretrain_coverage.items():\n            if not all(cov):\n                score = sum([not c for c in cov])\n                pretrain_problematic.append((prot, pretrain_measurements[prot], score))\n        \n        for prot, cov in finetune_coverage.items():\n            if not all(cov):\n                score = sum([not c for c in cov])\n                finetune_problematic.append((prot, finetune_measurements[prot], score))\n        \n        total_proteins = len(current_pretrain_proteins) + len(current_finetune_proteins)\n        perfect_coverage = len(pretrain_problematic) + len(finetune_problematic)\n        coverage_rate = 1.0 - (perfect_coverage / total_proteins) if total_proteins > 0 else 1.0\n        \n        print(f\"Iteration {iteration}: Coverage={coverage_rate:.3f}, Problematic={perfect_coverage}\")\n        \n        if coverage_rate >= target_coverage or perfect_coverage == 0:\n            break\n        \n        # Find protein to remove\n        all_problematic = []\n        for p, m, s in pretrain_problematic:\n            all_problematic.append((p, m, s, 'pretrain'))\n        for p, m, s in finetune_problematic:\n            all_problematic.append((p, m, s, 'finetune'))\n        \n        if all_problematic:\n            all_problematic.sort(key=lambda x: (x[1], -x[2]))\n            protein_to_remove = all_problematic[0][0]\n            proteins_to_remove.add(protein_to_remove)\n            print(f\"  Removing {protein_to_remove} ({all_problematic[0][1]} measurements)\")\n    \n    return df_pretrain_split, df_finetune_split, proteins_to_remove, current_pretrain_proteins, current_finetune_proteins\n\n# Run optimization\nprint(\"Starting optimization for 100% protein coverage...\")\ndf_pretrain_split, df_finetune_split, removed_proteins, final_pretrain_proteins, final_finetune_proteins = optimize_protein_coverage(\n    df_pretrain_orig, df_finetune_orig, pretrain_proteins, finetune_proteins, \n    protein_to_family, target_coverage=1.0\n)\n\n# Save pretraining splits\nfor split_val, name in enumerate(['train', 'val', 'test']):\n    mask = df_pretrain_split['split'] == split_val\n    orig_indices = df_pretrain_split[mask]['original_idx'].values\n    df_out = df_pretrain_orig.loc[df_pretrain_orig.index.isin(orig_indices), ['smiles'] + final_pretrain_proteins]\n    df_out.to_csv(f'datasets/chembl_pretraining_{name}.csv', index=False)\n\n# Save finetuning splits\nfor split_val, name in enumerate(['train', 'val', 'test']):\n    mask = df_finetune_split['split'] == split_val\n    orig_indices = df_finetune_split[mask]['original_idx'].values\n    df_out = df_finetune_orig.loc[df_finetune_orig.index.isin(orig_indices), ['smiles'] + final_finetune_proteins]\n    df_out.to_csv(f'datasets/pkis2_finetuning_{name}.csv', index=False)\n\n# Save split assignments\ndf_pretrain_split[['smiles', 'scaffold_smiles', 'split']].to_csv('split_pretrain.csv', index=False)\ndf_finetune_split[['smiles', 'scaffold_smiles', 'split']].to_csv('split_finetune.csv', index=False)\n\n# Calculate final statistics\npretrain_scaffolds_final = {\n    'train': set(df_pretrain_split[df_pretrain_split['split']==0]['scaffold_smiles'].dropna()),\n    'val': set(df_pretrain_split[df_pretrain_split['split']==1]['scaffold_smiles'].dropna()),\n    'test': set(df_pretrain_split[df_pretrain_split['split']==2]['scaffold_smiles'].dropna())\n}\n\nfinetune_scaffolds_final = {\n    'train': set(df_finetune_split[df_finetune_split['split']==0]['scaffold_smiles'].dropna()),\n    'val': set(df_finetune_split[df_finetune_split['split']==1]['scaffold_smiles'].dropna()),\n    'test': set(df_finetune_split[df_finetune_split['split']==2]['scaffold_smiles'].dropna())\n}\n\noverlap_val = pretrain_scaffolds_final['train'] & finetune_scaffolds_final['val']\noverlap_test = pretrain_scaffolds_final['train'] & finetune_scaffolds_final['test']\n\n# Print final results\nprint(f\"\\n=== OPTIMIZATION COMPLETE ===\")\nprint(f\"Removed {len(removed_proteins)} proteins to achieve 100% coverage:\")\nfor prot in sorted(removed_proteins):\n    print(f\"  - {prot}\")\n\nprint(f\"\\nFinal splits:\")\nprint(f\"  Pretraining: {(df_pretrain_split['split'] == 0).sum()}/{(df_pretrain_split['split'] == 1).sum()}/{(df_pretrain_split['split'] == 2).sum()}\")\nprint(f\"  Finetuning: {(df_finetune_split['split'] == 0).sum()}/{(df_finetune_split['split'] == 1).sum()}/{(df_finetune_split['split'] == 2).sum()}\")\n\nprint(f\"\\nOrthogonality check:\")\nprint(f\"  pretrain_train ∩ finetune_val: {len(overlap_val)} scaffolds\")\nprint(f\"  pretrain_train ∩ finetune_test: {len(overlap_test)} scaffolds\")\n\nprint(f\"\\nRemaining proteins:\")\nprint(f\"  Pretraining: {len(final_pretrain_proteins)}/{len(pretrain_proteins)} proteins\")\nprint(f\"  Finetuning: {len(final_finetune_proteins)}/{len(finetune_proteins)} proteins\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}